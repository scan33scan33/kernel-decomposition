\documentclass{beamer}
%\usetheme{Singapore}
\usetheme{Darmstadt}
\useoutertheme{smoothtree}
\setbeamertemplate{footline}[page number]{}
\beamertemplateballitem
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{bm}
\usepackage{array,amsmath,latexsym,epsfig,float,afterpage,alltt,amssymb,theorem,enumerate,pstricks,bm,hhline,url,xr,float,multicol,multirow,ragged2e,colortbl,xspace, natbib}
\graphicspath{{figs/}}

\newcommand{\Blue}{\color{blue}}
\newcommand{\Red}{\color{red}}
\newcommand{\Green}{\color{green}}
\def\1{{\bf 1}}\def\rkhs{{\cal H}}

% Example definitions.
% --------------------

\def\cL{{\cal L}}
\def\cU{{\cal U}}
\def\cF{{\cal F}}
\def\cM{{\cal M}}
\def\cC{{\cal C}}
\def\cD{{\cal D}}
\def\cA{{\cal A}}


\def\bR{{\boldsymbol R}}
\def\bS{{\boldsymbol S}}
\def\bJ{{\boldsymbol J}}
\def\ba{{\boldsymbol a}}
\def\bb{{\boldsymbol b}}
\def\bw{{\boldsymbol w}}
\def\bu{{\boldsymbol u}}
\def\balpha{{\boldsymbol \alpha}}
\def\bbeta{{\boldsymbol \beta}}
\def\bd{{\boldsymbol d}}
\def\bb{{\boldsymbol b}}
\def\be{{\boldsymbol e}}
\def\bp{{\boldsymbol p}}
\def\bzero{{\boldsymbol 0}}
\def\bx{{\boldsymbol x}}
\def\by{{\boldsymbol y}}
\def\bv{{\boldsymbol v}}
\def\bq{{\boldsymbol q}}
\def\bxi{{\boldsymbol \xi}}
\def\liblinear{{{\sf LIBLINEAR}\xspace}}
\def\vw{{\sf VW}\xspace}
\def\pegasos{{\sf PEGASOS}\xspace}
\def\libsvm{{\sf LIBSVM}\xspace}

\def\bX{{\mathbf X}}
\def\x{{\mathbf x}}
\def\L{{\cal L}}

\AtBeginSubsection[]
{
    \begin{frame}<beamer>
        \frametitle{Outline}
        \tableofcontents[current,currentsubsection]
    \end{frame}
}


\begin{document}
\section{ }
\title[Nystr\"om Decomposition]{Solving Non-Linear SVM in Linear Time? A Nystr\"om Approximated SVM with Applications to Image Classification}
\author[Ming-Hen Tsai]{
{\scriptsize
Ming-Hen Tsai, Academia Sinica, Taiwan (now at Google Inc.)\\
Joint work with \\
Yi-Ren Yeh, Intel-NTU Connected Context Computing Center \\
Yuh-Jye Lee, Dept. CSIE, National Taiwan University Science \& Technology\\
Yu-Chiang Frank Wang, Research Center for IT Innovation, Academia Sinica\\
}
}
\date{May 21, 2013}


%%-----------------------------------------------
%\begin{frame}
%\end{frame}


%-----------------------------------------------
\begin{frame}
\titlepage
\end{frame}

%%-----------------------------------------------
\begin{frame}{Outline}
\tableofcontents[pausesections]
\end{frame}

%-----------------------------------------------
\subsection{Introduction}

\begin{frame}
  \frametitle{Visual Classification: Faces, Objects, and Beyond}
  \begin{figure}
  % Requires \usepackage{graphicx}
  \includegraphics[width=4in]{tasks_illus.png}\\
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Example: Some Image Classification Data Sets}
  \begin{figure}
  % Requires \usepackage{graphicx}
  \includegraphics[width=4in]{tasks_summary.png}\\
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{We Have Known...}
  \begin{itemize}
    \item Non-linear SVM: powerful but slow 
    \item Linear SVM: simple but fast  
    \item [] Paper: ``Training Linear SVMs in Linear Time'' by Joachims KDD'06
    \item [] Software: \liblinear, \vw, etc.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{We Had Always Wondered...}
  \begin{itemize}
    \item But we want something {\bf powerful} and {\bf fast}: train faster than non-linear SVM and generate a more accurate model than linear SVM.
    \pause
    \item [] Is this even possible? 
    \pause
    \item [] Yes. Do approximation!
  \end{itemize}
\end{frame}


\subsection{The Nystr\"om Method for Kernel Approximation}
\begin{frame}
  \frametitle{Definitions and a Brief Review (1/2)}
  \begin{itemize}
    \item Input Data: $\{(\by, X)\} = \{(y_i, \bx_i)\}_{i=1}^\ell$. $y_i$: label, $\bx_i$: feature vector.
    \item (Non-zero) dimension of $\bx_i$: $d$
    \item Non-linear mapping $\phi(\bx)$ (e.g. bi-gram features)
    \item Kernel function: $K(\bu, \bv) = \phi(\bu)^T \phi(\bv)$, usually computed in linear time.
    \item For ease of representation, for $U = [\bu_1, \dots, \bu_\ell]$ and $V = [\bv_1, \dots, \bv_{\tilde{\ell}}]$, we define $Q = K(U, V)$, where $Q_{ij} = K(\bu_i, \bv_j)$
    % rv_1_henry: I use \tilde{\ell}  here because U and V can be of different sizes
    \pause
    \item Lower bound for data storage and training: $\Omega(\ell d)$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Definitions and a Brief Review (2/2)}
  \begin{itemize}
    \item Primal SVM:
    \begin{equation}
      \min_{\bw, b}
      \frac{1}{2} \bw^T\bw + C\sum_{i=1}^\ell \max(1-y_i\bw^T\phi(\bx_i), 0) \nonumber
    \end{equation} 
    \item Dual SVM: 
    \begin{align}
    \min_{\balpha} \  &  \frac{1}{2} \balpha^T Q  \balpha - \be^T \balpha \nonumber \\
    \mbox{s.t.} \  & 0 \le \alpha_i \le C \quad \forall i \mbox{.} \nonumber
    \end{align}
    \item Primal-Dual Correspondence: $\bw = \sum_{i=1}^\ell y_i \alpha_i \phi(\bx_i)$ 
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Kernel in Non-linear SVM}
  \begin{itemize}
    \item Dual form:
    \begin{align}
    \label{eq:svmdual} \nonumber
    \min_{\balpha} \  &  \frac{1}{2} \balpha^T Q  \balpha - \be^T \balpha \nonumber \\
    \mbox{s.t.} \  & 0 \le \alpha_i \le C \quad \forall i \mbox{.} \nonumber
    \end{align}
    \item Kernel matrix $Q$ with $Q_{ij} = K(\bx_i, \bx_j)$.
    \pause
    \item [] Computational time: $O(\ell^2 d) \sim \ell$ times data size.
    \item [] Space: $O(\ell^2)$.
    \pause
    \item $1M$ images: $1M$ times more computational time than the linear counterpart.
    % rv1_henry: Only list out time now.
%    \item $1M$ images, $1k$ dimensional features: $1M$ times more computational time, and $1M/1k = 1k$ times more space than the linear counterpart.
    % I dont have that resources!!!
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Nystr\"om Method}
  \begin{itemize}
    \item A low-rank kernel approximation method. (Why?)
    \pause
    \item Sample $\tilde{\ell} (\ll \ell)$ feature vectors from $X$ with a basis set $B = \{\bb_i\}_{i = 1}^{\tilde{\ell}}$.
    \item [] The low-rank apprxoimated kernel: $Q \sim \tilde{Q} = P^TW^{-1}P$, 
    where $P_{ij} = K(\bx_i, \bb_j)$ and $W_{ij} = K(\bb_i, \bb_j)$.
%    \item [] so $C$ is $\ell$ by $k$, $W$ is $k$ by $k$. Usually $n \sim k \ll \ell$. 
\begin{figure}
  \includegraphics[width=3in]{nystrom.pdf}\\
\end{figure}


    \pause
    \item Time: Basis Selection + $O(\ell^2 \max(\tilde{\ell}, d))$ overall. (Alright...)
    \item [] Space: $O(\ell \max(\tilde{\ell}, d))$. (Good! Don't know how large is $\tilde{\ell}$..)
    \pause
    \item Set our basis size, $\tilde{\ell}$ to $d$. The space consumption is optimal.
  \end{itemize}
\end{frame}

\subsection{The Nystr\"om Method for Linear SVM Classification}
\begin{frame}
  \frametitle{An Equivalent Representation (1/2)}
  \begin{itemize}
    \item Bottleneck: kernel matrix $Q$ of size $O(\ell^2)$.
    \item [] But observe the following...
    \pause
    \item $Q = \tilde{X}^T\tilde{X}$ by Cholesky decomposition. (This is valid because $Q$ is PSD by definition.)
    \item Investigate the columns of $\tilde{X} = [\tilde{\bx}_1 \dots \tilde{\bx}_\ell]$. We have 
    $
    \tilde{\bx_i}^T \tilde{\bx_j} = 
    Q_{ij} = 
    \phi(\bx_i)^T \phi(\x_j) = K(\bx_i,\bx_j) 
    $.
    \pause
    \item We represent the kernelized linear products as regular linear products. Let's call $\tilde{X}$ a compact representation. 
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{An Equivalent Representation (2/2)}
  \begin{itemize}
    \item SVM Dual: 
    \begin{align}
    \min_{\balpha} \  &  \frac{1}{2} \balpha^T Q  \balpha - \be^T \balpha \nonumber \\
    \mbox{s.t.} \  & 0 \le \alpha_i \le C \quad \forall i \mbox{.} \nonumber
    \end{align}
    \pause
    \item SVM Primal:
    \begin{equation}
      \min_{\bw, b}
      \frac{1}{2} \bw^T\bw + C\sum_{i=1}^\ell \max(1-y_i\bw^T\tilde{\bx}_i, 0) \nonumber
    \end{equation} 
    \pause
    \item Issue: computing $\tilde{\bx}_i$ requires the computation of the $\ell^2$-sized kernel.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Nystr\"oms' Equivalent Representation}
  \begin{itemize}
    \item Let's go back to kernel approximation!
    \pause
    \item As $\tilde{Q} = P^TW^{-1}P$, we can find a matrix $R$ such that $W^{-1} = R^TR$, and $RP$ is our a compact representation.
    \pause
    \item [] Time: Basis selection time + $O(\ell \tilde{\ell} d)$ overall.
    \item [] Space: $O(\ell \max(\tilde{\ell}, d))$.
    \pause
    \item Needs a linear feature selection method!
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Brief Recap}
%  \begin{itemize}
%  \item []
  \begin{figure}
    \includegraphics[width=3in]{nystrom.pdf}
  \end{figure}
%  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Another View of Basis Selection (1/2)}
  \begin{itemize}
    \item Recall that: $P_{:i} = [K(\bx_i, \bb_1), \dots, K(\bx_i, \bb_{\tilde{\ell}})]^T$
    \item Can we view $RP$ as input data and do feature selection?
    \pause
    \item No! $R$ introduces dependency between dimensions.
    \pause
    \item We remove $R$! Each dimension is then independent of other basis vector.
    \item L1-regularized linear SVM which runs linear time in data size is used in the work.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Another View of Basis Selection (2/2)}
  \begin{itemize}
    \item Does it hurt by removing $R$?
    \pause
    \item Say $\bp_+$ and $\bp_-$ are two samples with different labels.
    \pause
    \item If for a $\bw$, $\bw^T\bp_+ > \bw^T\bp_-$ (separable), then
    $\bw^TR^{-1}R\bp_+  > \bw^TR^{-1}R\bp_-$.
    \pause
    \item $R^{-1}\bw$ separates $R\bp_+$ and $R\bp_-$.
    \pause
    \item When doing the linear tranformation by $R$, we preserve separability.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The Algorithm Workflow}
  \begin{enumerate}
    \item Input data: $(\by, X)$, a kernel $K$.
    \item Run basis selection on $K(X, B)$.
    \item Compute the new data $\tilde{X} = RK(X, B)$, where $ R^TR =  K(B, B)^{-1}$.
    \item Train on $(\by, \tilde{X})$
    \pause
    \item [] If we use $\tilde{\ell}$ samples to do 2....
    \pause
    \item [] Time: $O(\ell \tilde{\ell} d)$ overall.
    \item [] Space: $O(\ell \max(\tilde{\ell}, d))$.
  \end{enumerate}
\end{frame}

%-----------------------------------------------
\subsection{Experiments and Conclusions}
%-----------------------------------------------
%-----------------------------------------------
\begin{frame}{Experimental Settings}
\begin{itemize}
  \item We conduct experiments on two benchmark datasets: USPS and MNIST.
  \item We randomly split 70\% of the data for training and the remaining 30\% for testing. (Repeat 5 times)
  \item We perform a five-fold cross validation to select the parameters $\gamma$ and $C$.
\end{itemize}
\begin{table}
\caption{Dataset descriptions (with the number $\ell$ of instances and the dimension $d$ of the data). The sizes for storing the data $\ell d$ and the associated kernel matrices $\ell^2$ are also listed.}
\center{
%\scriptsize
%\small
\begin{tabular}{|l|l|l|l|l|}
\hline
      & $\ell$ & $d$ & kernel size $\ell^2$ & data size $\ell d$ \\
\hline
USPS & 7291 & 256 &   53M  & 2M \\
MNIST   & 60000  & 780  & 3.6G & 47M  \\
\hline
\end{tabular}
}
\label{tab:datasets}
\end{table}
\end{frame}



%-----------------------------------------------
\begin{frame}
\frametitle{Compare Accuracy between Different Types of Methods}
\begin{itemize}
  \item Our method achieved improved accuracy than linear SVMs
  \item The time for training and testing using our proposed model is comparable to that of linear SVMs
  \item The standard nonlinear SVM utilizes the full kernel matrix whose time complexity is quadratically scaled-up with $\ell$.
\end{itemize}

\begin{table}
{\small
\center{
  \scriptsize
\begin{tabular}{|l|l|l|l|}
\hline
USPS  & Accuracy & Training Time & Testing Time   \\
\hline
Nystr\"om primal SVM & $97.057 \pm 0.402$ & $3.764 $ & $0.320 $   \\
nonlinear SVM & $98.007 \pm 0.198$ & $14.507 $ & $4.129 $   \\
linear SVM & $95.009 \pm 0.275$ & $2.274 $ & $0.079 $   \\
\hline
MNIST  & Accuracy & Training Time & Testing Time   \\
\hline
Nystr\"om primal SVM & $93.833 \pm 0.115$ & $24.6092 $ & $2.006 $   \\
nonlinear SVM & $98.547 \pm 0.066$ & $3650.334 $ & $524.487 $   \\
linear SVM & $91.917 \pm 0.077$ & $14.510 $ & $0.381 $   \\
\hline
\end{tabular}
}
\label{tab:mrkl_with_others}
}\end{table}
\end{frame}

%-----------------------------------------------
\begin{frame}
\frametitle{Comparisons to Random Basis Selection}
\begin{itemize}
  \item Compare basis matrix determined by our method to
  \item the random sampling basis selection strategy
\end{itemize}
\begin{figure}[h!]
\centering
  \includegraphics[width=6cm]{compare.jpg}
\label{fig:select}
\end{figure}

\end{frame}

%-----------------------------------------------
\begin{frame}{Conclusion}
\begin{itemize}
  \item Primal low-rank representation of Nytr\"om-approximated dual SVM.
  \item [] Training: linearly in time and space.
  \item [] Accuracy: almost as good as non-linear SVM.
  \item [] Prediction: nearly as fast as linear SVM. (e.g. Realtime robot vision applications.)
  \item Connect feature selection with basis selection in Nystr\"om method. 
  \item [] A basis selection method that preserves separability.
  \item Applications: Large-Scale Image Retrieval, Realtime Machine Vision (training/test should be nearly as fast as feature extraction,) etc.
\end{itemize}
\end{frame}
\end{document} 
